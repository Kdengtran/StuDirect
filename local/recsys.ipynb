{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# clean tokens\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "import re\n",
    "\n",
    "from helper import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user profiles\n",
    "important_user_columns = ['Row ID', 'Woonplaats', 'School', 'Studie', 'Studie Jaar', 'Ambitie', 'Core Values', \n",
    "                          'Industry Interest', 'Technical Skills', 'Social Skills', 'Desired Skills']\n",
    "\n",
    "users = pd.read_csv('Main Users.csv')[important_user_columns].set_index('Row ID')\n",
    "\n",
    "# job profiles\n",
    "important_job_columns = ['ðŸ”’ Row ID', 'Job Title', 'Type of Contract', 'Industry', 'Place', 'Where', 'Salary per hour',\n",
    "                         'Hours per week', 'Language', 'Education', 'WhatYouLearn', 'WhatYouDo', 'WhyUs',\n",
    "                         'WhoAreYou', 'Core Values']\n",
    "\n",
    "jobs = pd.read_csv('Vacatures.csv')[important_job_columns].set_index('ðŸ”’ Row ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the actual Glide data to test the functions\n",
    "# dummy_data = pythonanywhere_api_call(\"Kevin Tran; University of Amsterdam; Data Science; Python, \\\n",
    "#     SQL, Machine Learning, DevOps Engineering\")\n",
    "\n",
    "# # connect to spreadsheets using json credentials downloaded from Google Cloud\n",
    "# gc = gspread.service_account(filename='credentials.json')\n",
    "\n",
    "# # open the worksheets and select the first; we don't have other worksheets\n",
    "# sh = gc.open_by_key('1T2If_xR-fhQw6hFejDxdPLLnz1J0lDstTKZ1FJVNwQI') # extract key from https\n",
    "# worksheet = sh.sheet1\n",
    "\n",
    "# # fetch data and return a Pandas DataFrame\n",
    "# results = worksheet.get_all_records()\n",
    "# job_df = pd.DataFrame(results)\n",
    "# job_df.set_index(job_df.columns[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    \n",
    "    # make sure we don't fuck up anything\n",
    "    profile = df.copy()\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        # add spaces after the comma; only necessary for user profiles\n",
    "        extra_whitespace = ['Core Values', 'Industry Interest', 'Technical Skills', 'Social Skills', 'Desired Skills']\n",
    "        profile[extra_whitespace] = profile[extra_whitespace].str.replace(',', ', ')\n",
    "\n",
    "    except:\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # remove English stopwords and lowercase all tokens\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    cleaned = [str(word).lower() for word in profile if word not in english_stopwords]\n",
    "\n",
    "    # stem words of the cleaned list\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in cleaned]\n",
    "    \n",
    "    # transform into string \n",
    "    output = \" \".join(token for token in stemmed_words)\n",
    "    \n",
    "    # remove punctuation, unnecesarry whitespaces and new lines\n",
    "    output = output.translate(str.maketrans('', '', string.punctuation)).replace('\\n', ' ').replace('  ', ' ')\n",
    "    \n",
    "    return output\n",
    "\n",
    "def match_and_rank(user, jobs, top_n):\n",
    "\n",
    "    # create local student variable\n",
    "    student = clean(user)\n",
    "\n",
    "    # student ID, job ID, and the cosine similarity score\n",
    "    rank_list = []\n",
    "    \n",
    "    bow_model = TfidfVectorizer()\n",
    "\n",
    "    # iterate over all the jobs and match with the local student variable\n",
    "    for index, job in jobs.iterrows():\n",
    "        tf_idf = bow_model.fit_transform([student, clean(job)])\n",
    "        rank_list.append((job.name, jobs.loc[job.name]['Job Title'], cosine_similarity(tf_idf)[0][1].round(2)))\n",
    "\n",
    "    # return the top n results --> Job title: Similarity score\n",
    "    sorted_list = sorted(rank_list, key=lambda s: s[2], reverse=True)\n",
    "    \n",
    "    return sorted_list[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mlearning-ai/enhancing-information-retrieval-via-semantic-and-relevance-matching-64973ff81818\n",
    "\n",
    "1. Create matrices for both the user and job profiles\n",
    "2. Fit a bag-of-words based model (e.g. TF-IDF)\n",
    "3. Calculate the cosine similarity\n",
    "4. Rank the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k6zDYaC5RiW85McgaCzYPA', 'Data Engineer', 0.08),\n",
       " ('I-NrC.VDRfaZSo-Az6izug', 'Data & Analytics Consultant', 0.07),\n",
       " ('aX84O.QlSjKUuSfDWM23Ng', 'Business Analyst', 0.06),\n",
       " ('xpEOOL14Rv-YvdNPjp1cFQ', 'Data & Analytics Internship', 0.06),\n",
       " ('lyqjIAkxS2qYlpEpZsqggg', 'Commercial Intern London', 0.06)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_and_rank(users.iloc[0], jobs, 5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "88eebfa414ed89cca1c6c238e663e30f5c39b3bf12a2e49deefc8609ac1c2ce4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
